import os
import json5
import faiss
import numpy as np
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from config import DATA_DIR, ARK_API_KEY, ARK_BASE_URL

client = OpenAI(api_key=ARK_API_KEY, base_url=ARK_BASE_URL)

def load_json(file_name):
    try:
        file_path = os.path.join(DATA_DIR, file_name)
        with open(file_path, "r", encoding="utf-8") as f:
            return json5.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {file_name} é”™è¯¯: {e}")
        return None

def load_schema():
    schema_path = os.path.join(DATA_DIR, "data_schema.json5")
    return load_json(schema_path)

def get_relevant_files(user_question, schema):
    prompt = "\n".join([f"{k}: {v['description']}" for k, v in schema.items()])
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæ•°æ®æ–‡ä»¶çš„ç”¨é€”æè¿°ï¼Œåˆ¤æ–­éœ€è¦è¯»å–å“ªäº›æ•°æ®æ–‡ä»¶ã€‚åªè¿”å› JSON æ•°ç»„ï¼Œä¾‹å¦‚ï¼š[\"recent_remarks.json\", \"bigFive.json\"]"},
        {"role": "user", "content": f"ç”¨æˆ·é—®é¢˜ï¼š{user_question}\n\næ•°æ®æ–‡ä»¶æè¿°å¦‚ä¸‹ï¼š\n{prompt}"}
    ]
    try:
        response = client.chat.completions.create(
            model="doubao-pro-32k-241215",
            messages=messages
        )
        raw_content = response.choices[0].message.content.strip()
        print(f"ğŸ“ æ¨¡å‹åŸå§‹è¿”å›å†…å®¹ï¼š{raw_content}")
        # è§£æä¸ºJSONæ•°ç»„
        files = json5.loads(raw_content)
        print(f"ğŸ“ æ¨¡å‹åˆ¤æ–­ç›¸å…³æ–‡ä»¶ï¼š{files}")
        return files
    except Exception as e:
        print(f"âŒ æ–‡ä»¶é€‰æ‹©å¤±è´¥ï¼š{e}")
        return []



def entry_to_text(entry, file_name):
    try:
        if file_name == "recent_remarks.json":
            return f"{entry.get('person', '')} çš„è¨€è®º: {entry.get('remark', '')}"
        elif file_name == "recent_news.json":
            return f"{entry.get('person', '')} çš„æ–°é—»: {entry.get('title_cn', '')}"
        elif file_name == "bigFive.json":
            scores = ", ".join([f"{k}:{v}" for k, v in entry.items()])
            return f"äººç‰©å¤§äº”äººæ ¼å¾—åˆ†: {scores}"
        elif file_name == "values.json":
            values = ", ".join([f"{k}:{v}" for k, v in entry.items()])
            return f"äººç‰©ä»·å€¼è§‚ç‰¹å¾: {values}"
        else:
            return json5.dumps(entry, ensure_ascii=False,) # ç¡®ä¿ä¸­æ–‡ä¸è¢«è½¬ä¹‰
    except Exception:
        return str(entry)

class MultiFaissRetriever:
    def __init__(self):
        self.texts = []
        self.metadata = []
        self.vectorizer = None
        self.index = None

    def load_files(self, file_list):
        self.texts = []
        self.metadata = []
        for file_name in file_list:
            file_path = os.path.join(DATA_DIR, file_name)
            data = load_json(file_path)
            if not data:
                continue
            if isinstance(data, dict):
                data = [data]
            for i, item in enumerate(data):
                text = entry_to_text(item, file_name)
                self.texts.append(text)
                self.metadata.append({"source": file_name, "index": i, "text": text})

        if self.texts:
            self.vectorizer = TfidfVectorizer(stop_words="english")
            self.matrix = self.vectorizer.fit_transform(self.texts).toarray().astype(np.float32)
            faiss.normalize_L2(self.matrix)
            self.index = faiss.IndexFlatIP(self.matrix.shape[1])
            self.index.add(self.matrix)
            print(f"âœ… æ£€ç´¢å™¨åŠ è½½å®Œæˆï¼Œå…± {len(self.metadata)} æ¡æ•°æ®")
        else:
            print("âš ï¸ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ•°æ®")
            self.vectorizer = None
            self.index = None

    def search(self, question, top_k=6):
        if not self.index or not self.vectorizer:
            return []
        q_vec = self.vectorizer.transform([question]).toarray().astype(np.float32)
        faiss.normalize_L2(q_vec)
        D, I = self.index.search(q_vec, top_k)
        return [self.metadata[i] for i in I[0] if i < len(self.metadata)]

def ask_model(user_question, evidence):
    context = "\n".join([f"{e['text']}ï¼ˆæ¥è‡ª {e['source']} ç¬¬ {e['index']} æ¡ï¼‰" for e in evidence])
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å‹ AI åŠ©æ‰‹ï¼Œç”¨æˆ·å°†æé—®ï¼Œä½ å°†è·å¾—è‹¥å¹²ä¸é—®é¢˜ç›¸å…³çš„æ•°æ®å†…å®¹ã€‚"
        "è¯·æ ¹æ®è¿™äº›å†…å®¹ç”Ÿæˆè¯¦ç»†ä¸”æœ‰é€»è¾‘çš„å›ç­”ï¼Œå¦‚ä¿¡æ¯ä¸è¶³å¯è¿›è¡Œåˆç†è¡¥å……æ¨ç†ã€‚"
    )
    user_prompt = f"é—®é¢˜ï¼š{user_question}\n\nç›¸å…³æ•°æ®å†…å®¹ï¼š\n{context}"

    try:
        response = client.chat.completions.create(
            model="doubao-pro-32k-241215",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}"

def answer_question(question: str, top_k: int = 6, debug: bool = False):
    schema = load_schema()
    if not schema:
        return {"error": "æ— æ³•åŠ è½½ schema æ–‡ä»¶"}

    relevant_files = get_relevant_files(question, schema)
    if not relevant_files:
        return {"error": "æœªèƒ½è¯†åˆ«ç›¸å…³æ–‡ä»¶"}

    retriever = MultiFaissRetriever()
    retriever.load_files(relevant_files)
    top_evidence = retriever.search(question, top_k=top_k)
    answer = ask_model(question, top_evidence)

    result = {
        "question": question,
        "answer": answer,
        "used_files": relevant_files,
    }
    if debug:
        result["evidence"] = top_evidence
    return result